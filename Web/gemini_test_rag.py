#!/usr/bin/env python3
# rag_gemini_chat.py (íšŒë¡œ ë¶„ì„ ê°•í™” ë²„ì „)

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import numpy as np
import pandas as pd
import faiss
import google.generativeai as genai
from sentence_transformers import SentenceTransformer

class RAGSystem:
    def __init__(self, index_path=r"D:/Hyuntak/lab/AR_circuit_tutor/breadboard_project/faiss_index.index", metadata_path=r"D:/Hyuntak/lab/AR_circuit_tutor/breadboard_project/embedding_metadata.parquet"):
        """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.index = None
        self.metadata = None
        self.embedding_model = None
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•´ì•¼ í•¨)
        print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            self.embedding_model = SentenceTransformer("intfloat/multilingual-e5-large-instruct")
            # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ (ì„ íƒ ì‚¬í•­)
            # import torch
            # if torch.cuda.is_available():
            #     print(f"âœ… SentenceTransformerê°€ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {torch.cuda.get_device_name(0)}")
            # else:
            #     print("âš ï¸ SentenceTransformerê°€ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("pip install sentence-transformers ë¡œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise

        # FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.load_database()
    
    def load_database(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            if os.path.exists(self.index_path):
                self.index = faiss.read_index(self.index_path)
                print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
            else:
                raise FileNotFoundError(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.index_path}")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            if os.path.exists(self.metadata_path):
                self.metadata = pd.read_parquet(self.metadata_path)
                print(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ ë¬¸ì„œ")
            else:
                raise FileNotFoundError(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.metadata_path}")
            
            # ë°ì´í„° ì¼ê´€ì„± í™•ì¸
            if self.index.ntotal != len(self.metadata):
                print(f"âš ï¸ ê²½ê³ : ì¸ë±ìŠ¤ ë²¡í„° ìˆ˜({self.index.ntotal})ì™€ ë©”íƒ€ë°ì´í„° í–‰ ìˆ˜({len(self.metadata)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("FAISS ì¸ë±ìŠ¤ íŒŒì¼ê³¼ ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise
    
    def search_similar_documents(self, query, top_k=5):
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
            # 1) GPU tensorë¡œ ì„ë² ë”© ìƒì„± (convert_to_tensor=True ì‹œ)
            query_embedding = self.embedding_model.encode(
                [query],
                convert_to_tensor=True,
                normalize_embeddings=True
            )
            # 2) CPUë¡œ ê°€ì ¸ì™€ì„œ numpy ë³€í™˜
            query_embedding = query_embedding.cpu().detach().numpy().astype('float32')
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(query_embedding, top_k)
            
            # ê²°ê³¼ ì •ë¦¬
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(self.metadata):  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
                    doc_info = self.metadata.iloc[idx].to_dict()
                    results.append({
                        'rank': i + 1,
                        'similarity_score': float(1 / (1 + distance)),  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë°©ë²•)
                        'distance': float(distance),
                        'content': doc_info.get('chunk_text', ''),
                        'metadata': doc_info # ì›ë³¸ ë©”íƒ€ë°ì´í„° ì „ì²´ë¥¼ í¬í•¨
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def create_context(self, search_results, max_length=3000): # max_length ì•½ê°„ ì¦ê°€
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not search_results:
            return ""
        
        context_parts = []
        current_length = 0
        
        for result in search_results:
            content = result['content']
            
            # ê¸¸ì´ ì œí•œ í™•ì¸
            if current_length + len(content) > max_length:
                # ë‚¨ì€ ê³µê°„ë§Œí¼ë§Œ ì¶”ê°€
                remaining_space = max_length - current_length
                if remaining_space > 200:  # ìµœì†Œ 200ìëŠ” ìˆì–´ì•¼ ì˜ë¯¸ê°€ ìˆìŒ
                    content = content[:remaining_space] + "..." # ì˜ë¦° ë¶€ë¶„ì„ í‘œì‹œ
                    context_parts.append(f"[ë¬¸ì„œ {result['rank']}] {content}")
                break # ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            
            context_parts.append(f"[ë¬¸ì„œ {result['rank']}] {content}")
            current_length += len(content)
        
        return "\n\n".join(context_parts)

def initialize_gemini():
    """Gemini APIë¥¼ ì´ˆê¸°í™”í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤."""
    # ì¤‘ìš”: ì‹¤ì œ API í‚¤ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.
    # ì•ˆì „ì„ ìœ„í•´ í™˜ê²½ ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    api_key = os.getenv("GOOGLE_API_KEY", "YOUR_GEMINI_API_KEY_HERE") # í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥

    if api_key == "YOUR_GEMINI_API_KEY_HERE" or not api_key:
        print("ê²½ê³ : GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì½”ë“œ ë‚´ í•˜ë“œì½”ë”©ëœ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print("ë˜ëŠ” 'export GOOGLE_API_KEY='your_api_key''ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        # ì•ˆì „ìƒì˜ ì´ìœ ë¡œ ì—¬ê¸°ì— ì§ì ‘ API í‚¤ë¥¼ ë„£ëŠ” ê²ƒì€ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì•„ë˜ ë¼ì¸ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤ì œ í‚¤ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
        api_key = "AIzaSyCxgQUQFLhTi-Y6nzRpdpgpgHO9xVJ-CAo" 
    
    genai.configure(api_key=api_key)

    generation_config = genai.GenerationConfig(
        temperature=0.4, # ì˜¨ë„ê°’ì„ ì¡°ê¸ˆ ë” ë‚®ì¶°ì„œ ì•ˆì •ì ì¸ ì‘ë‹µ ìœ ë„ (ê¸°ì¡´ 0.7ì—ì„œ ë³€ê²½)
        max_output_tokens=2000 # ì¶œë ¥ í† í° ìˆ˜ ì¦ê°€ (ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´)
    )

    model_names = [
        "models/gemini-2.5-flash", # ë” ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ flash ëª¨ë¸ ìš°ì„  ê³ ë ¤
        "models/gemini-2.5-pro",   # ë” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ìœ„í•´ pro ëª¨ë¸ ê³ ë ¤
    ]

    for model_name in model_names:
        try:
            print(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘: {model_name}")
            model = genai.GenerativeModel(model_name)
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
            test_response = model.generate_content(
                "ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”? ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”.",
                generation_config=generation_config
            )
            if test_response.text:
                print(f"âœ… {model_name} ì‚¬ìš© ê°€ëŠ¥!")
                return model, generation_config
            else:
                print(f"âŒ {model_name} ì‘ë‹µ ì—†ìŒ (text í•„í„°ë§ë¨).")
        except Exception as e:
            print(f"âŒ {model_name} ì‚¬ìš© ë¶ˆê°€: {e}")
            continue
    
    raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ì™€ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# gemini_test_rag.py íŒŒì¼ì˜ create_rag_prompt í•¨ìˆ˜ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.

def create_rag_prompt(user_query, context, is_first_turn, practice_circuit_topic=""):
    """
    RAGìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (íšŒë¡œ ë¶„ì„ ê°•í™” ë° ê°„ê²°í•œ ë‹µë³€ ìœ ë„).
    Args:
        user_query (str): ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸.
        context (str): ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© ë˜ëŠ” íšŒë¡œ ë¶„ì„ ê²°ê³¼.
        is_first_turn (bool): í˜„ì¬ í„´ì´ ì²« ë²ˆì§¸ í„´ì¸ì§€ ì—¬ë¶€.
        practice_circuit_topic (str): ì‹¤ìŠµ íšŒë¡œì˜ ì£¼ì œ.
    """
    
    # ì»¨í…ìŠ¤íŠ¸ì—ì„œ íšŒë¡œ ë¶„ì„ ê²°ê³¼ ì—¬ë¶€ í™•ì¸
    has_circuit_analysis = any(keyword in context for keyword in [
        "=== íšŒë¡œ ë¶„ì„ ê²°ê³¼ ===", "ìœ ì‚¬ë„ ì ìˆ˜", "ê°ì§€ëœ ì˜¤ë¥˜", "ì„±ëŠ¥ í‰ê°€", "SPICE ë„·ë¦¬ìŠ¤íŠ¸"
    ])
    
    if is_first_turn:
        # ì²« ë²ˆì§¸ í„´: íšŒë¡œ ë¶„ì„ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ (ê°„ê²°í•˜ê²Œ)
        if has_circuit_analysis:
            prompt = f"""ë‹¹ì‹ ì€ ì „ì íšŒë¡œ ë¶„ì„ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì œê³µëœ íšŒë¡œ ë°ì´í„°ë¥¼ ê²€í† í•˜ê³  **í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ** ë¶„ì„í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ì§€ì¹¨:**
- ê° í•­ëª©ì„ 2-3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.
- ì „ë¬¸ ìš©ì–´ëŠ” ìµœì†Œí™”í•˜ê³ , ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**ë‹µë³€ í˜•ì‹ (ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜):**
1. **ğŸ” ì¢…í•© í‰ê°€:** íšŒë¡œì˜ ì™„ì„±ë„ì™€ ì‹¤ìŠµ ëª©í‘œ ë‹¬ì„±ë„ì— ëŒ€í•œ ì´í‰
2. **âš ï¸ ì£¼ìš” ë¬¸ì œì :** ê°€ì¥ ì‹œê¸‰í•˜ê²Œ ìˆ˜ì •í•´ì•¼ í•  ì˜¤ë¥˜ 1~2ê°œ
3. **ğŸ› ï¸ í•µì‹¬ ê°œì„  ë°©ì•ˆ:** ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê°€ì¥ ì¤‘ìš”í•œ ì œì•ˆ
4. **ğŸ“Š ê¸°ì¤€ íšŒë¡œ ë¹„êµ:** '{practice_circuit_topic}' íšŒë¡œì™€ì˜ ìœ ì‚¬ë„ ë° ì£¼ìš” ì°¨ì´ì 

--- íšŒë¡œ ë°ì´í„° ë° ë¶„ì„ ê²°ê³¼ ---
{context}

--- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ---
{user_query}

--- ì „ë¬¸ê°€ì˜ ê°„ê²°í•œ íšŒë¡œ ë¶„ì„ ---
"""
        else:
            # ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (ê°„ê²°í•˜ê²Œ)
            prompt = f"""ë‹¹ì‹ ì€ ì „ì íšŒë¡œ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íšŒë¡œì˜ íŠ¹ì§•ê³¼ '{practice_circuit_topic}' ì‹¤ìŠµ ëª©í‘œì™€ì˜ ì—°ê´€ì„±ì„ **ê°„ê²°í•˜ê²Œ** ì„¤ëª…í•´ì£¼ì„¸ìš”.

--- ì œê³µëœ íšŒë¡œ ì •ë³´ ---
{context}

--- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ---
{user_query}

--- ì „ë¬¸ê°€ì˜ ê¸°ë³¸ ë¶„ì„ (ìš”ì•½) ---
"""
    else:
        # ì´í›„ í„´: ì¼ë°˜ì ì¸ Q&A (ê°„ê²°í•˜ê²Œ)
        if context.strip():
            prompt = f"""ë‹¹ì‹ ì€ ì „ì íšŒë¡œ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— **í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ** ë‹µë³€í•´ì£¼ì„¸ìš”.

- í˜„ì¬ ì‹¤ìŠµ ì£¼ì œ: '{practice_circuit_topic}'
- ë‹µë³€ì€ í•­ìƒ 1~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

--- ì°¸ê³  ìë£Œ ---
{context}

--- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ---
{user_query}

--- ì „ë¬¸ê°€ì˜ ë‹µë³€ (ìš”ì•½) ---
"""
        else:
            prompt = f"""ë‹¹ì‹ ì€ ì „ì íšŒë¡œ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— **í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ** ë‹µë³€í•´ì£¼ì„¸ìš”.

- í˜„ì¬ ì‹¤ìŠµ ì£¼ì œ: '{practice_circuit_topic}'
- ë‹µë³€ì€ í•­ìƒ 1~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

--- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ---
{user_query}

--- ì „ë¬¸ê°€ì˜ ë‹µë³€ (ìš”ì•½) ---
"""

    return prompt


def chat_with_rag():
    """RAG ì‹œìŠ¤í…œê³¼ Geminië¥¼ ê²°í•©í•œ ì±„íŒ… í•¨ìˆ˜ (íšŒë¡œ ë¶„ì„ ê°•í™”)"""
    # 1) RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    try:
        rag_system = RAGSystem()
    except Exception as e:
        print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤: {e}")
        return

    # 2) Gemini ëª¨ë¸ ì´ˆê¸°í™”
    print("Gemini ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    try:
        model, generation_config = initialize_gemini()
    except Exception as e:
        print(f"Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤: {e}")
        return
    
    # === ì‚¬ìš©ì ì •ì˜ ì„¤ì • ===
    # ì‹¤ìŠµ íšŒë¡œì˜ ì£¼ì œë¥¼ ì—¬ê¸°ì— ëª…ì‹œì ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    practice_circuit_topic = "Op-Amp Inverting Amplifier (ë°˜ì „ ì¦í­ê¸°)" 
    # ======================

    first_turn = True
    
    print("\n" + "="*60)
    print("ğŸ¤– RAG + Gemini íšŒë¡œ ë¶„ì„ ì±„íŒ… ì‹œì‘! (ì¢…ë£Œ: quit/exit/ì¢…ë£Œ)")
    print("============================================================")
    print(f"ğŸ’¡ í˜„ì¬ ì‹¤ìŠµ ì£¼ì œ: {practice_circuit_topic}")
    print("============================================================")
    
    while True:
        user_input = input("\në‹¹ì‹ : ").strip()
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
            print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break
        if not user_input:
            print("âŒ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        
        # 3) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        print("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...", end=" ", flush=True)
        search_results = rag_system.search_similar_documents(user_input, top_k=3)
        print(f"({len(search_results)}ê°œ ë¬¸ì„œ ë°œê²¬)")
        
        # 4) ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = rag_system.create_context(search_results)
        
        # 5) ê°•í™”ëœ RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
        rag_prompt = create_rag_prompt(user_input, context, first_turn, practice_circuit_topic)

        # ì²« í„´ ì´í›„ì—ëŠ” first_turnì„ Falseë¡œ ì„¤ì •
        if first_turn:
            first_turn = False

        # 6) Geminië¡œ ë‹µë³€ ìƒì„±
        print("ğŸ¤– AI íšŒë¡œ ì „ë¬¸ê°€: ", end="", flush=True)
        try:
            response = model.generate_content(
                rag_prompt,
                generation_config=generation_config
            )
            print(response.text)
        except genai.types.BlockedPromptException as e:
            # BlockedPromptExceptionì€ ì•ˆì „ í•„í„°ì— ì˜í•œ ì°¨ë‹¨ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            print(f"âŒ ìƒì„± ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ê°€ ì•ˆì „ ì •ì±…ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. (finish_reason: {e.response.prompt_feedback.block_reason})")
            print("ì´ì „ ì§ˆë¬¸, ê²€ìƒ‰ëœ ë¬¸ì„œ(ì»¨í…ìŠ¤íŠ¸), ë˜ëŠ” í˜„ì¬ ì§ˆë¬¸ì— ë¶€ì ì ˆí•˜ê±°ë‚˜ ìœ í•´í•˜ë‹¤ê³  íŒë‹¨ë  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            print(f"ìì„¸í•œ ì‘ë‹µ: {e.response}") # ë” ìì„¸í•œ ì •ë³´ ì¶œë ¥
        except Exception as e:
            print(f"âŒ ìƒì„± ì˜¤ë¥˜: {e}")
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ (e.g., ë„¤íŠ¸ì›Œí¬, API í‚¤ ë¬¸ì œ ë“±)
            if "finish_reason" in str(e) and "2" in str(e):
                print("ì´ëŠ” ì£¼ë¡œ ì•ˆì „ í•„í„°(Safety Filter)ì— ì˜í•´ ì‘ë‹µì´ ì°¨ë‹¨ë  ë•Œ ë°œìƒí•©ë‹ˆë‹¤.")
                print("í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë‚´ìš©(ì§ˆë¬¸, ì»¨í…ìŠ¤íŠ¸)ì— ë¬¸ì œê°€ ì—†ëŠ”ì§€ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.")
            print(f"ë°œìƒí•œ ì˜¤ë¥˜ ìœ í˜•: {type(e)}")

        # 7) ì°¸ê³  ë¬¸ì„œ ì •ë³´ í‘œì‹œ (ì˜µì…˜)
        if search_results:
            print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(search_results)}ê°œ (ìµœê³  ìœ ì‚¬ë„ {search_results[0]['similarity_score']:.3f})")

def main():
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    required_packages = ['faiss-cpu', 'sentence-transformers', 'pandas', 'numpy', 'google-generativeai']
    missing_packages = []
    
    try:
        import faiss
        import sentence_transformers
        import pandas
        import numpy
        import google.generativeai
    except ImportError as e:
        missing_module = str(e).split("'")[1]
        if missing_module == 'faiss':
            missing_packages.append('faiss-cpu')
        elif missing_module == 'google': # google.generativeai
            missing_packages.append('google-generativeai')
        else:
            missing_packages.append(missing_module)
    
    if missing_packages:
        print("âŒ ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        for pkg in missing_packages:
            print(f"   pip install {pkg}")
        return
    
    chat_with_rag()

if __name__ == "__main__":
    main()